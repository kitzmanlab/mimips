# eg
# from your data directory:
# snakemake -s PATH_TO/preprocess_and_align.snake --configfile PATH_TO/config.yaml --config sample_table=key_test.txt work_dir=`pwd` prefix=mymipset  --printshellcmds  --cores 16

# must specify configuration values:
#   sample_table - must be a file w/ columns: libname, fq_fwd, fq_rev
#   prefix - a name for this set of mip captures.

import os.path as op
import os
import pandas as pd
import re


########

OUT_DIR = config['work_dir']

########
# load and check sample table.

tblSamples = pd.read_csv( config['sample_table'], sep='\t' )

assert all( [ col in tblSamples.columns for col in ('libname','fq_fwd','fq_rev') ] ), 'sample table must have columns: libname, fq_fwd, fq_rev'

assert tblSamples[['libname']].drop_duplicates().shape[0]==tblSamples.shape[0], 'each library name must be unique'

tblSamples = tblSamples.set_index( ['libname'], drop=False )

lLibs = tblSamples.libname.unique()

########
# expected output files

assert 'prefix' in config, 'must specify a value for prefix (eg --config prefix="mymips")'

# final output fastqs
lOut = expand('{}/temp/fqfix/{{libname}}.1.fq.gz'.format(OUT_DIR),
              libname=lLibs)
lOut += expand('{}/temp/fqfix/{{libname}}.2.fq.gz'.format(OUT_DIR),
              libname=lLibs)

# sample_key
OUT_STRIP_RPT = '{prefix}.adaptstrip.txt'.format(OUT_DIR=OUT_DIR, prefix=config['prefix'])
lOut += [ OUT_STRIP_RPT ]

rule all:
    input:
        lOut

########
# generate bed file of targets for coverage analysis

########

# map and consolidate by library
rule cutadapt:  
    input:
        fq_fwd=lambda wc:list(tblSamples.loc[ wc ][ 'fq_fwd' ]),
        fq_rev=lambda wc:list(tblSamples.loc[ wc ][ 'fq_rev' ])
    output:
        fq_fwd_out=OUT_DIR+'/temp/fqfix/{libname}.1.fq.gz',
        fq_rev_out=OUT_DIR+'/temp/fqfix/{libname}.2.fq.gz'
    params:
        ca_log=OUT_DIR+'/temp/fqfix/{libname}.cutadapt.log'
    threads: 1
    run:
        shell(
        """
        cutadapt -a GTCTTCGCCCTAGATCCTGCA \
            -e 0.2 -O 20 \
            --action none \
            --discard-trimmed \
            -o {output.fq_fwd_out} \
            -p {output.fq_rev_out} \
            {input.fq_fwd} \
            {input.fq_rev} \
            1>{params.ca_log}
         """
         )

rule output_key:
    input:
        fq1_input=expand(rules.cutadapt.output.fq_fwd_out,libname=lLibs),
        fq2_input=expand(rules.cutadapt.output.fq_rev_out,libname=lLibs),
    output:
        out_rpt=OUT_STRIP_RPT,
    run:
        tbl_out = tblSamples.copy()
        tbl_out['fq_fwd_input'] = tbl_out['fq_fwd']
        tbl_out['fq_rev_input'] = tbl_out['fq_rev']
        tbl_out['fq_fwd'] = list(input.fq1_input)
        tbl_out['fq_rev'] = list(input.fq2_input)

        tbl_out['n_pairs_pre_adaptscreen']=0
        tbl_out['n_pairs_pass_adaptscreen']=0
        tbl_out['frac_pairs_pass_adaptscreen']=0

        for ln in lLibs:
            fn_log = OUT_DIR+'/temp/fqfix/{}.cutadapt.log'.format(ln)
            ll_log = [ l.strip() for l in open(fn_log,'r') ]
            ll_tm = [ re.search( r'Total read pairs processed:\s+([0-9,]+)', l ) for l in ll_log ]
            ll_tm = [ m for m in ll_tm if m]
            assert(len(ll_tm)==1,ln)

            nin = int( ll_tm[0].group(1).replace(',','') )
            tbl_out.loc[ln,'n_pairs_pre_adaptscreen'] = nin

            ll_wi = [ re.search( r'Pairs written \(passing filters\):\s+([0-9,]+)\s+\(([0-9.]+)%\)', l ) for l in ll_log ]
            ll_wi = [ m for m in ll_wi if m ]
            assert(len(ll_wi)==1,ln)

            nout = int( ll_wi[0].group(1).replace(',','') )
            tbl_out.loc[ln,'n_pairs_pass_adaptscreen'] = nout

            if nout>0:
                tbl_out.loc[ln,'frac_pairs_pass_adaptscreen'] = nout / nin
            else:
                tbl_out.loc[ln,'frac_pairs_pass_adaptscreen'] = 0


        tbl_out.to_csv( output.out_rpt, sep='\t', index=False )
